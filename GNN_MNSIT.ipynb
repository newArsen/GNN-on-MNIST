{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81pqgCd02m_I",
        "outputId": "e7ebdb0a-d9b0-42e3-de80-351a7eb93310"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting torch_geometric\n",
            "  Downloading torch_geometric-2.5.3-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m15.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (4.66.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.11.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2023.6.0)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.3)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.9.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.31.0)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.2.2)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->torch_geometric) (4.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2024.2.2)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (1.4.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->torch_geometric) (3.4.0)\n",
            "Installing collected packages: torch_geometric\n",
            "Successfully installed torch_geometric-2.5.3\n"
          ]
        }
      ],
      "source": [
        "!pip install torch_geometric"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FYMV1OQP2m_J"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import time\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision.transforms as transforms\n",
        "from torch_geometric.nn import GATConv\n",
        "from torch_geometric.datasets.mnist_superpixels import MNISTSuperpixels\n",
        "from torch_geometric.data import Data\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import torchvision\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xDCSYQa32m_K"
      },
      "outputs": [],
      "source": [
        "in_chls = 3\n",
        "num_classes = 10\n",
        "num_epochs = 2\n",
        "learning_rate = 3e-3\n",
        "batch_size = 64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9GhLzNXw2m_K"
      },
      "outputs": [],
      "source": [
        "transform = transforms.Compose([transforms.ToTensor()])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5503tOK2m_K"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1WeX0TqR2m_K",
        "outputId": "6033378a-f3c7-47b4-b740-4d290a278de6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://data.pyg.org/datasets/MNISTSuperpixels.zip\n",
            "Extracting mnist-superpixels-dataset/raw/MNISTSuperpixels.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "def build_collate_fn(device: str | torch.device):\n",
        "    def collate_fn(original_batch: list[Data]):\n",
        "        batch_node_features: list[torch.Tensor] = []\n",
        "        batch_edge_indices: list[torch.Tensor] = []\n",
        "        classes: list[int] = []\n",
        "\n",
        "        for data in original_batch:\n",
        "            node_features = torch.cat((data.x, data.pos), dim=-1).to(device)\n",
        "            edge_indices = data.edge_index.to(device)\n",
        "            class_ = int(data.y)\n",
        "            batch_node_features.append(node_features)\n",
        "            batch_edge_indices.append(edge_indices)\n",
        "            classes.append(class_)\n",
        "\n",
        "        collated = {\"batch_node_features\": batch_node_features, \"batch_edge_indices\": batch_edge_indices, \"classes\": torch.LongTensor(classes).to(device)}\n",
        "\n",
        "        return collated\n",
        "    return collate_fn\n",
        "\n",
        "graph_dataset = MNISTSuperpixels(root=\"mnist-superpixels-dataset\", train=False)\n",
        "\n",
        "train_dataset = MNISTSuperpixels(root=\"mnist-superpixels-dataset\", train=True)\n",
        "test_dataset = MNISTSuperpixels(root=\"mnist-superpixels-dataset\", train=False)\n",
        "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [0.8, 0.2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bkj8ow142m_L"
      },
      "outputs": [],
      "source": [
        "gnn_train_loader = DataLoader(train_dataset, batch_size, shuffle=True, collate_fn=build_collate_fn(device=device))\n",
        "gnn_val_loader = DataLoader(val_dataset, batch_size, shuffle=False, collate_fn=build_collate_fn(device=device))\n",
        "gnn_test_loader = DataLoader(test_dataset, batch_size, shuffle=False, collate_fn=build_collate_fn(device=device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zQ3SRiUq2m_L"
      },
      "source": [
        "# GNN Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zkr9525u2m_L"
      },
      "outputs": [],
      "source": [
        "class GNN(nn.Module):\n",
        "    def __init__(self, in_chls, hidden_dim, num_classes):\n",
        "        super().__init__()\n",
        "        self.in_channels = in_chls\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.num_classes = num_classes\n",
        "        self.conv1 = GATConv(in_chls, hidden_dim)\n",
        "        self.conv2 = GATConv(hidden_dim, hidden_dim)\n",
        "        self.conv3 = GATConv(hidden_dim, hidden_dim)\n",
        "        self.conv4 = GATConv(hidden_dim, hidden_dim)\n",
        "        self.l1 = nn.Linear(in_chls+4*hidden_dim, 256)\n",
        "        self.l2 = nn.Linear(256, 128)\n",
        "        self.l3 = nn.Linear(128, num_classes)\n",
        "        self.relu = nn.ReLU(True)\n",
        "\n",
        "    def forward_one_base(self, node_features, edge_indices):\n",
        "        assert node_features.ndim == 2 and node_features.shape[1] == self.in_channels\n",
        "        assert edge_indices.ndim == 2 and edge_indices.shape[0] == 2\n",
        "\n",
        "        s0 = node_features\n",
        "        s1 = self.conv1(s0, edge_indices)\n",
        "        s2 = self.conv2(s1, edge_indices)\n",
        "        s3 = self.conv3(s2, edge_indices)\n",
        "        s4 = self.conv4(s3, edge_indices)\n",
        "        s0_s1_s2_s3_s4 = torch.cat((s0, s1, s2, s3, s4), dim=-1)\n",
        "        return s0_s1_s2_s3_s4\n",
        "\n",
        "    def forward(self, batch_node_features, batch_edge_indices):\n",
        "        assert len(batch_node_features) == len(batch_edge_indices)\n",
        "        features_list = []\n",
        "        for node_features, edge_indices in zip(batch_node_features, batch_edge_indices):\n",
        "            features_list.append(self.forward_one_base(node_features=node_features, edge_indices=edge_indices))\n",
        "\n",
        "        features = torch.stack(features_list, dim=0)\n",
        "        features = features.mean(dim=1)\n",
        "        logits = nn.ReLU()(self.l1(features))\n",
        "        logits = nn.ReLU()(self.l2(logits))\n",
        "        logits = self.l3(logits)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v-Vcdzck2m_L"
      },
      "outputs": [],
      "source": [
        "GNNmodel = GNN(in_chls, 152, num_classes).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFBWt3_Z2m_L"
      },
      "source": [
        "# Training Loop GNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IDqAnhTo2m_M"
      },
      "outputs": [],
      "source": [
        "def trainGNN(model, learning_rate, num_epochs):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    optimizer = optim.Adam(model.parameters(), learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    start_time = time.time()\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        tr_loss = 0.0\n",
        "        for batch in gnn_train_loader:\n",
        "            node_features, edge_features = batch['batch_node_features'], batch['batch_edge_indices']\n",
        "            logits = model(node_features, edge_features)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(logits, batch['classes'])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            tr_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            vl_loss = 0.0\n",
        "            for batch in gnn_val_loader:\n",
        "                node_features, edge_features = batch['batch_node_features'], batch['batch_edge_indices']\n",
        "                logits = model(node_features, edge_features)\n",
        "                loss = criterion(logits, batch['classes'])\n",
        "                vl_loss += loss.item()\n",
        "\n",
        "        train_losses.append(tr_loss/len(gnn_train_loader))\n",
        "        val_losses.append(vl_loss/len(gnn_val_loader))\n",
        "        print(f'epoch : {epoch+1}/{num_epochs} || train loss : {train_losses[-1]} || validation loss : {val_losses[-1]}')\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Training completed in {elapsed_time} seconds\")\n",
        "\n",
        "    return model, train_losses, val_losses, elapsed_time_gnn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "wktFXQh79jUa",
        "outputId": "18088e7c-e3ba-4da3-d83b-6f644620bcf1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch : 1/2 || train loss : 2.304510418256124 || validation loss : 2.3003661569128644\n",
            "epoch : 2/2 || train loss : 2.3018108517328897 || validation loss : 2.300649809076431\n",
            "Training completed in 850.387220621109 seconds\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'elapsed_time_gnn' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-1bf9e3c6274f>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrained_gnn_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses_gnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses_gnn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melapsed_time_gnn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainGNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGNNmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-11-e72630827bda>\u001b[0m in \u001b[0;36mtrainGNN\u001b[0;34m(model, learning_rate, num_epochs)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training completed in {elapsed_time} seconds\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melapsed_time_gnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'elapsed_time_gnn' is not defined"
          ]
        }
      ],
      "source": [
        "trained_gnn_model, train_losses_gnn, val_losses_gnn, elapsed_time_gnn = trainGNN(GNNmodel, learning_rate, num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1-qiYQN2m_M"
      },
      "source": [
        "# Hyperparameter Tuning for GNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yj_9ywF82m_M"
      },
      "outputs": [],
      "source": [
        "import itertools\n",
        "learning_rates = [0.003, 0.001]\n",
        "hidden_dims = [64, 128]\n",
        "best_val_accuracy = 0.0\n",
        "best_model_gnn = None\n",
        "\n",
        "for learning_rate, hidden_dim in itertools.product(learning_rates, hidden_dims):\n",
        "\n",
        "    model = GNN(3, hidden_dim, num_classes).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    print(f\"\\nTraining with learning rate: {learning_rate}, hidden dimension: {hidden_dim}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    for epoch in range(2):\n",
        "        model.train()\n",
        "        for batch in tqdm(gnn_train_loader, desc=f\"Epoch {epoch+1} - Training\"):\n",
        "            node_features, edge_features = batch['batch_node_features'], batch['batch_edge_indices']\n",
        "            optimizer.zero_grad()\n",
        "            output = model(node_features, edge_features)\n",
        "            loss = criterion(output, batch['classes'])\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Evaluate on validation set\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(gnn_val_loader, desc=f\"Epoch {epoch+1} - Validation\"):\n",
        "            node_features, edge_features = batch['batch_node_features'], batch['batch_edge_indices']\n",
        "            output = model(node_features, edge_features)\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            total += len(node_features)\n",
        "            correct += (predicted == batch['classes']).sum().item()\n",
        "\n",
        "    val_accuracy = correct / total\n",
        "\n",
        "    if val_accuracy > best_val_accuracy:\n",
        "        best_val_accuracy = val_accuracy\n",
        "        best_model_gnn = model\n",
        "\n",
        "    print(f\"Validation Accuracy: {val_accuracy}, Time: {time.time() - start_time}s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkCpc7Ua2m_M"
      },
      "outputs": [],
      "source": [
        "plt.plot(train_losses_gnn, label='Training Loss')\n",
        "plt.plot(val_losses_gnn, label='Validation Loss')\n",
        "plt.title('Loss Curves GNN', weight='bold')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1jarY8C2m_N"
      },
      "source": [
        "# Dataset Split CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "srWgyOK02m_N"
      },
      "outputs": [],
      "source": [
        "train_dataset_cnn = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset_cnn = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-o3hfp3L2m_N"
      },
      "outputs": [],
      "source": [
        "train_size = int(0.8 * len(train_dataset_cnn))\n",
        "val_size = len(train_dataset_cnn) - train_size\n",
        "train_dataset_cnn, val_dataset_cnn = random_split(train_dataset_cnn, [train_size, val_size])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-vqu9vV2m_N"
      },
      "outputs": [],
      "source": [
        "cnn_train_loader = DataLoader(train_dataset_cnn, batch_size=batch_size, shuffle=True)\n",
        "cnn_val_loader = DataLoader(val_dataset_cnn, batch_size=batch_size, shuffle=False)\n",
        "cnn_test_loader = DataLoader(test_dataset_cnn, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wOWUx8Sz2m_N"
      },
      "source": [
        "# CNN Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNY5gMuZ2m_N"
      },
      "outputs": [],
      "source": [
        "class CNN_block(nn.Module):\n",
        "    def __init__(self, in_chls, out_chls, kernel_size):\n",
        "        super(CNN_block, self).__init__()\n",
        "        self.block = nn.Sequential(\n",
        "            nn.Conv2d(in_chls, out_chls, kernel_size),\n",
        "            nn.BatchNorm2d(out_chls),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.block(x)\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, in_chls, num_classes):\n",
        "        super(CNN, self).__init__()\n",
        "        self.in_chls = in_chls\n",
        "        self.num_classes = num_classes\n",
        "        self.nblocks = nn.Sequential(\n",
        "            CNN_block(in_chls, 16, 3),\n",
        "            CNN_block(16, 32, 3),\n",
        "            CNN_block(32, 64, 3),\n",
        "            CNN_block(64, 128, 3)\n",
        "        )\n",
        "\n",
        "        self.fc = nn.Linear(128*20*20, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.nblocks(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.fc(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ANOLK3pS2m_N"
      },
      "outputs": [],
      "source": [
        "CNNmodel = CNN(1, num_classes).to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvUDVumF2m_N"
      },
      "source": [
        "# Training Loop CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU3ob5n82m_N"
      },
      "outputs": [],
      "source": [
        "def trainCNN(model, learning_rate, num_epochs):\n",
        "    train_losses_cnn = []\n",
        "    val_losses_cnn = []\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    start_time = time.time()\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        tr_loss = 0.0\n",
        "        for batch in cnn_train_loader:\n",
        "            img, label = batch\n",
        "            img = img.to(device)\n",
        "            label = label.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            logits = model(img)\n",
        "            loss = criterion(logits, label)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            tr_loss += loss.item()\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            vl_loss = 0.0\n",
        "            for batch in cnn_val_loader:\n",
        "                img, label = batch\n",
        "                img = img.to(device)\n",
        "                label = label.to(device)\n",
        "                logits = model(img)\n",
        "                loss = criterion(logits, label)\n",
        "                vl_loss += loss.item()\n",
        "\n",
        "        train_losses_cnn.append(tr_loss/len(cnn_train_loader))\n",
        "        val_losses_cnn.append(vl_loss/len(cnn_val_loader))\n",
        "        print(f'epoch : {epoch+1}/{num_epochs} || train loss : {train_losses_cnn[-1]} || validation loss : {val_losses_cnn[-1]}')\n",
        "\n",
        "    end_time = time.time()\n",
        "    elapsed_time = end_time - start_time\n",
        "    print(f\"Training completed in {elapsed_time} seconds\")\n",
        "\n",
        "    return model, train_losses_cnn, val_losses_cnn, elapsed_time_cnn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rdCnvzcR2m_N"
      },
      "outputs": [],
      "source": [
        "trained_cnn, train_losses_cnn, val_losses_cnn, elapsed_time_cnn = trainCNN(CNNmodel, learning_rate, num_epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6sviikE2m_O"
      },
      "source": [
        "# Vizualising Images in Graph Format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oscnJqIS2m_O"
      },
      "outputs": [],
      "source": [
        "def create_superpixel_image(record, scale=30, edge_width=1):\n",
        "    pos = (record.pos.clone() * scale).int()\n",
        "    image = np.zeros((scale * 26, scale * 26, 1), dtype=np.uint8)\n",
        "\n",
        "    # Draw rectangles for each superpixel\n",
        "    for color, (x, y) in zip(record.x, pos):\n",
        "        x0, y0 = int(x), int(y)\n",
        "        x1, y1 = x0 - scale, y0 - scale\n",
        "        color = min(int(float(color + 0.15) * 255), 255)\n",
        "        cv2.rectangle(image, (x0, y0), (x1, y1), color, -1)\n",
        "\n",
        "    # Draw edges between superpixels\n",
        "    for node_ix_0, node_ix_1 in record.edge_index.T:\n",
        "        x0, y0 = list(map(int, pos[node_ix_0]))\n",
        "        x1, y1 = list(map(int, pos[node_ix_1]))\n",
        "        x0 -= scale // 2\n",
        "        y0 -= scale // 2\n",
        "        x1 -= scale // 2\n",
        "        y1 -= scale // 2\n",
        "        cv2.line(image, (x0, y0), (x1, y1), 125, edge_width)\n",
        "\n",
        "    return image\n",
        "\n",
        "def visualize_superpixels(dataset, examples_per_class=5, classes=tuple(range(10)), figsize=(30, 50), edge_width=1):\n",
        "    class_to_examples = {class_ix: [] for class_ix in classes}\n",
        "\n",
        "    # Collect examples for each class\n",
        "    for record in dataset:\n",
        "        class_ix = int(record.y)\n",
        "        if class_ix not in class_to_examples or len(class_to_examples[class_ix]) >= examples_per_class:\n",
        "            continue\n",
        "        class_to_examples[class_ix].append(create_superpixel_image(record, edge_width=edge_width))\n",
        "\n",
        "    # Plot the collected examples\n",
        "    plt.figure(figsize=figsize)\n",
        "    for i, class_ix in enumerate(classes):\n",
        "        for j, example in enumerate(class_to_examples[class_ix]):\n",
        "            plt.subplot(len(classes), examples_per_class, i * examples_per_class + j + 1)\n",
        "            plt.imshow(example, cmap=plt.cm.binary)\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QfaPDEOG2m_O"
      },
      "outputs": [],
      "source": [
        "visualize_superpixels(graph_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pt60bKrb2m_O"
      },
      "source": [
        "# Comparing Performance of GNN and CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVXyRyC72m_O"
      },
      "outputs": [],
      "source": [
        "def evaluate_gnn(model, test_loader, device):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch in gnn_test_loader:\n",
        "            node_features, edge_features = batch['batch_node_features'], batch['batch_edge_indices']\n",
        "            logits = model(node_features, edge_features)\n",
        "            _, predicted = torch.max(logits.data, 1)\n",
        "            total += len(node_features)\n",
        "            correct += (predicted == batch['classes']).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UHnJH4r42m_O"
      },
      "outputs": [],
      "source": [
        "def evaluate_cnn(model, test_loader, device):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = 100 * correct / total\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62th9lYI2m_U"
      },
      "outputs": [],
      "source": [
        "gnn_accuracy = evaluate_gnn(best_model_gnn, gnn_test_loader, device)\n",
        "print(f\"The accuracy of GNN is : {gnn_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MXjsgDjS2m_U"
      },
      "outputs": [],
      "source": [
        "cnn_accuracy = evaluate_cnn(CNNmodel, cnn_test_loader, device)\n",
        "print(f\"The accuracy of CNN is : {cnn_accuracy}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 30699,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}